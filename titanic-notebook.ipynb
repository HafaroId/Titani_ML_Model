{"metadata":{"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#import section\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sb\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, MinMaxScaler, PowerTransformer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#read train and test files as pandas dataframes\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\n\n#first look at the datasets\ntrain.head()\ntest.head()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Pclass                                          Name     Sex  \\\n0          892       3                              Kelly, Mr. James    male   \n1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n2          894       2                     Myles, Mr. Thomas Francis    male   \n3          895       3                              Wirz, Mr. Albert    male   \n4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n\n    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n0  34.5      0      0   330911   7.8292   NaN        Q  \n1  47.0      1      0   363272   7.0000   NaN        S  \n2  62.0      0      0   240276   9.6875   NaN        Q  \n3  27.0      0      0   315154   8.6625   NaN        S  \n4  22.0      1      1  3101298  12.2875   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Let's get some info about data in datasets\nnum_features = [col for col in train.columns if train[col].dtype in ['int64', 'float64']] # numeric features\ncat_features = [col for col in train.columns if train[col].dtype == 'object'] # ategorical features\nprint(num_features)\nprint(cat_features)\nprint('______________________________________________________', '\\n')\n\n#Let's figure out the amount of missing and unique values in each column of train and test\ntrain_info = pd.DataFrame(index = train.columns)\ntrain_info['Amount_of_miss_val'] = [train[col].isnull().sum() for col in train.columns] # absolute amount of missing values\ntrain_info['Miss_val_percent'] = train_info['Amount_of_miss_val']*100/train.shape[0] # missing values percentage\ntrain_info['Unique_values'] = [train[col].nunique() for col in train.columns] # amount of unique values\nprint('Train:', train_info)\nprint('______________________________________________________', '\\n')\ntest_info = pd.DataFrame(index = test.columns)\ntest_info['Amount_of_miss_val'] = [test[col].isnull().sum() for col in test.columns] # absolute amount of missing values\ntest_info['Miss_val_percent'] = test_info['Amount_of_miss_val']*100/test.shape[0] # missing values percentage\ntest_info['Unique_values'] = [test[col].nunique() for col in test.columns] # amount of unique values\nprint('Test:', test_info)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n______________________________________________________ \n\nTrain:              Amount_of_miss_val  Miss_val_percent  Unique_values\nPassengerId                   0          0.000000            891\nSurvived                      0          0.000000              2\nPclass                        0          0.000000              3\nName                          0          0.000000            891\nSex                           0          0.000000              2\nAge                         177         19.865320             88\nSibSp                         0          0.000000              7\nParch                         0          0.000000              7\nTicket                        0          0.000000            681\nFare                          0          0.000000            248\nCabin                       687         77.104377            147\nEmbarked                      2          0.224467              3\n______________________________________________________ \n\nTest:              Amount_of_miss_val  Miss_val_percent  Unique_values\nPassengerId                   0          0.000000            418\nPclass                        0          0.000000              3\nName                          0          0.000000            418\nSex                           0          0.000000              2\nAge                          86         20.574163             79\nSibSp                         0          0.000000              7\nParch                         0          0.000000              8\nTicket                        0          0.000000            363\nFare                          1          0.239234            169\nCabin                       327         78.229665             76\nEmbarked                      0          0.000000              3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's look on featrures with a lot of unique values\nprint('\\n Name:', train['Name'],\n      '\\n__________________________________________',\n      '\\nTicket:', train['Ticket'],\n      '\\n__________________________________________',\n      '\\nFare:', train['Fare'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As a result of first look on datasets we'll drop 'PassengerId' and 'Ticket' columns \n# because they don't affect at final prediction at all since have a lot of unique values\n# From the 'Name' feature we'll try to mine title of each person \ntrain = train.drop(['PassengerId', 'Ticket'], axis=1)\ntest_df = test.drop(['PassengerId', 'Ticket'], axis=1)\n\n# We'll round 'Fare' feature\ntrain['Fare'] = round(train['Fare'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's provide some EDA on our data\n\nplt.figure(figsize=(10,6))\n# At first let's look on our target data - \"Survived\" column\nsb.countplot(x=train.Survived.map({0:'Not-Survived', 1:'Survived'}))\nSurv_amount = round(len(train.loc[train['Survived']==1])/len(train)*100, 1)\nprint(\"Amount of survived people\", Surv_amount, '%') # Almost 62% of people was not survived\n\n# At further let's look at each feature effect\n# 'Sex' feature\nplt.figure(figsize=(10,6))\nsb.countplot(x=train.Sex) # Males are almost as twice as females\nplt.figure(figsize=(10,6))\nsb.barplot(x=train.Sex, y=train['Survived']) # Females have much more chances to survive\n\n# 'Pclass' feature\nsb.countplot(x=train.Pclass) # Most of the peoples travelled by 3-rd class\nplt.figure(figsize=(10,6))\nsb.barplot(x=train.Pclass, y=train['Survived']) # But the higher the class the higher survival chance rate\n\n# 'Embarked' feature represent the port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\nplt.figure(figsize=(10,6))\nsb.countplot(x=train.Embarked) \nplt.figure(figsize=(10,6))\nsb.barplot(x=train.Embarked, y=train['Survived']) # People from Cherbourg seems to have a little more chances to Survive\n\n# 'Age' feature\nplt.figure(figsize=(10,6))\nsb.countplot(x=train['Age']) # Most of the people are 20-40 years old\nplt.figure(figsize=(10,6))\nsb.barplot(x=train['Age'], y=train['Survived']) # Seems like childs and aged people have more chances to survive\n\n# 'Fare' feature\nplt.figure(figsize=(10,6))\nsb.countplot(x=train['Fare']) \nplt.figure(figsize=(10,6))\nsb.boxplot(x=train['Survived'], y=train['Fare']) # Seems like in average who paid more have more chances to survive\n\n# 'SibSp' feature. Represents № of siblings / spouses\nplt.figure(figsize=(10,6))\nsb.countplot(x=train['SibSp']) # Most of the people had no siblings/spouses\nplt.figure(figsize=(10,6))\nsb.barplot(x=train['SibSp'], y=train['Survived']) # People with one or two sib/sp have more chances to survive\n\n# 'Parch' feature. Represents № of parents / childs\nplt.figure(figsize=(10,6))\nsb.countplot(x=train['Parch']) # Similar to 'SibSp' mos pf the people travelled alone\nplt.figure(figsize=(10,6))\nsb.barplot(x=train['Parch'], y=train['Survived']) # People with 1,2 or 3 par/ch have more chances to survive\n\n# 'Cabin' feature. We'll skip this feature because of the hig amount of unique values\n\nplt.figure(figsize=(10,6))\n#At the final  let's look at correlation heatmap\nsb.heatmap(train.corr(), annot=True) # Acording to coorrelation coeff. 'Pclass' have the strongest effect on target, \n                                    #however we don't know for sure if there is a strong linear relation in this ds\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's make some simple Feature engineering\n# Let's try to mine title from 'Name' feature\ntrain['Title'] = train.Name.str.split(' ').str[1]\ntest_df['Title'] = test_df.Name.str.split(' ').str[1]\nprint('Number of unique titles:', train['Title'].nunique())\nprint(train['Title'].head())\nprint('_______________________________________________', '\\n', '\\n')\n\n# Here we are trying to get Cabin class from 'Cabin' feature\ntrain['Cabin_class'] = train.Cabin.str[0]\ntest_df['Cabin_class'] = test_df.Cabin.str[0]\nprint('Number of unique Cabin_class:', train['Cabin_class'].nunique())\nprint(train['Cabin_class'].head())\nprint('_______________________________________________', '\\n', '\\n')\n\ntrain = train.drop(['Name', 'Cabin'], axis=1) # we don't need 'Name' and 'Cabin' features anymore\ntest_df = test_df.drop(['Name', 'Cabin'], axis=1)\n\n# And now let's look on our train set\nprint(train.head()) # we have 10 features (1 target)\nprint('_______________________________________________', '\\n', '\\n')\nprint(train.info()) # 3 features ('Age', 'Embarked', 'Cabin_class') have missing values, 4 features are categorical\nprint('_______________________________________________', '\\n', '\\n')\n\nprint(test_df.head()) \nprint('_______________________________________________', '\\n', '\\n')\nprint(test_df.info())\nprint('_______________________________________________', '\\n', '\\n')\n\n# Let's assign our train test without target to X, our target - Survived - to y\nX = train.drop(['Survived'], axis=1)\ny = train['Survived']\nprint(X.head)\nprint('_______________________________________________', '\\n', '\\n')\nprint(y.head())\n# Other data preparation we'll do it by using Scikit Pipeline inside GridSearch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"   ** **Now after simple feature engineering our dataset still contain missing values and categorical values, thus, we need to preprocess dataset before model building. All of that stuff, like handling missing values, encoding categorical features we will do using Pipeline inside of GridSearch. So what we are going to do is to construct two separate preprocessing Pippelines for numerical and categorical columns, then we'll combine these Pipelines into one preprocessor by ColumnTransformer. Then we'll create another Pipeline that will contain our preprocessor and some model. Thus our final Pipeline will be able to prepare our dataset and model it. \n    However, we want to find the best prerossesing approaches and best model for this dataset, so we will pass our final Pipeline to GridSearch and provide it with a bunch of different preprocessing steps and model hyperparameters. As a result GridSearch will find the best preprocessing parameters and best model hyperparameters. In order to test different models we are going to create several GridSearch estimators with different models and after that we will find can find the best model with optimized hyperparameters.****","metadata":{}},{"cell_type":"code","source":"# At first let's define pipeline_constructor function that constructs preprocessor for handling our date before modeling\ndef pipeline_constructor():\n    \"\"\"\n    Construct Pipelines for numerical and categorical columns.\n    Create self.preprocessor and self.preprocessor_dict for final Pipeline and further GridSearch\n    \"\"\"\n\n    # At first we'll find columns with numerical and categorical values\n    num_features = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n    cat_features = [col for col in X.columns if X[col].dtype == 'object']\n\n    # Pipeline for numerical features that contain imputing, scaling and normalization operations\n    num_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', 'passthrough'),\n        ('norm', 'passthrough')\n    ])\n\n    # Pipeline for categorical features that contain imputing and one-hot encoding operations\n    cat_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n    # Combining of num and cat Pipelines into one preprocessor step using ColumnTransformer.\n    # This preprocessor will be used in final Pipeline and further in GridSearch\n    preprocessor = ColumnTransformer(transformers=[\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ])\n\n    # Creating of dictionary with preprocessing parameters. This dict will be used in GridSearch\n    preprocessor_dict = dict(preprocessor__num__imputer__strategy=['mean', 'median'],\n                             preprocessor__num__scaler=[StandardScaler(), RobustScaler(),\n                                                        RobustScaler(with_centering=False), MinMaxScaler()],\n                                preprocessor__num__norm=[PowerTransformer()])\n    return preprocessor, preprocessor_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we'll define three function that will be able to provide GridSearch with different models.\n# GridSearch also will use above defined preprocessor: \n\npreprocessor, preprocessor_dict =  pipeline_constructor()\n\ndef RFC_grid_search(grid_params={}):\n    \"\"\"\n    Method create classifier using preprocessor and RandomForestClassifier and fit this Pipeline to X, y\n\n    \"\"\"\n    RFC_classifier = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier())\n    ])\n    RFC_dict = dict(classifier__n_estimators=[100, 500, 900],  # classifier hyperparameters dict\n                    classifier__min_samples_leaf=[1, 2, 4])\n\n    grid_params.update(preprocessor_dict)\n    grid_params.update(RFC_dict) # dict used in GridSearch as param_grid\n\n    grid_search = GridSearchCV(RFC_classifier, param_grid=grid_params, scoring='roc_auc', n_jobs=-1)\n    grid_search.fit(X, y)\n\n    return grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_\n    \ndef SVC_grid_search(grid_params={}):\n    \"\"\"\n    Method create classifier using preprocessor and SVC and fit this Pipeline to X, y\n\n    \"\"\"\n    SVC_classifier = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', SVC(probability=True))\n    ])\n    SVC_dict = dict(classifier__kernel=['rbf', 'linear', 'sigmoid'],  # classifier hyperparameters dict\n                    classifier__C=[0.001, 0.01, 0.1, 1])\n    grid_params.update(preprocessor_dict)\n    grid_params.update(SVC_dict)\n\n    grid_search = GridSearchCV(SVC_classifier, param_grid=grid_params, scoring='roc_auc', n_jobs=-1)\n    grid_search.fit(X, y)\n    return grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_\n\ndef SGD_grid_search(grid_params={}):\n    \"\"\"\n    Method create classifier using preprocessor and SGD and fit this Pipeline to X, y\n\n    \"\"\"\n    SGD_classifier = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', SGDClassifier(early_stopping=True))\n    ])\n    SGD_dict = dict(classifier__alpha=[0.001, 0.01, 0.1, 1])  # classifier hyperparameters dict\n    grid_params.update(preprocessor_dict)\n    grid_params.update(SGD_dict)\n\n    grid_search = GridSearchCV(SGD_classifier, param_grid=grid_params, scoring='roc_auc', n_jobs=-1)\n    grid_search.fit(X, y)\n    return grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_\n\n# Finaly we'll create function that will be choose the best GridSearh estimator from the above defined\ndef grid_search_estimator(estimators_dict={}):\n    \"\"\"\n    Method choose best Pipeline basing on best_score\n\n    \"\"\"\n    rfc_score, rfc_best_estimator, rfc_best_params_ = RFC_grid_search()\n    svc_score, svc_best_estimator, svc_best_params_ = SVC_grid_search()\n    sgd_score, sgd_best_estimator, sgd_best_params_ = SGD_grid_search()\n\n    estimators_dict['RFC'] = [rfc_score, rfc_best_estimator]\n    estimators_dict['SV'] = [svc_score, svc_best_estimator]\n    estimators_dict['SGD'] = [sgd_score, sgd_best_estimator]\n    score_list = [i[0] for i in list(estimators_dict.values())]\n\n    f = 0\n    max_idx = 0\n    for j in range(len(score_list)):\n        if score_list[j] > f:\n            f = score_list[j]\n            max_idx = j\n\n    # max_idx = list.index(max(estimators_list))\n    print('best_estimator:', list(estimators_dict.keys())[max_idx],\n            '\\nbest_score:', list(estimators_dict.values())[max_idx][0])\n    if max_idx == 0:\n        return rfc_best_estimator, rfc_score\n    elif max_idx == 1:\n        return svc_best_estimator, svc_score\n    else:\n        return sgd_best_estimator, sgd_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's find teh best GridSearh estimator\nbest_estimator, best_score = grid_search_estimator()\nfitted_model = best_estimator.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the best GridSearh estimator we will predict our test dataset\ntest_pred = fitted_model.predict(test_df)\nprint(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': test_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\nprint(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}